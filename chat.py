# app.py
import os
import json
import uuid
from typing import List, Tuple, Generator, Optional

import gradio as gr
from loguru import logger
from openai import OpenAI

from prompt import big5_system_prompts_en, SYSTEM_PROMPT
from predictor import llmClient
import time
# ==============================
# Loguruï¼šJSON è¡Œæ—¥å¿— + è½®è½¬
# ==============================
logger.remove()
logger.add(
    "chat_history.jsonl",
    rotation="10 MB",
    retention="7 days",
    compression="zip",
    encoding="utf-8",
    enqueue=True,
    backtrace=False,
    diagnose=False,
    level="INFO",
    format="{message}",  # ç›´æ¥å†™ JSON è¡Œ
)

def log_json(event: str, **kwargs):
    logger.info(json.dumps({"event": event, **kwargs}, ensure_ascii=False))


# ==============================
# OpenAI å®¢æˆ·ç«¯
# ==============================

client = llmClient(
    model="gpt-4o-mini",
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url=os.getenv("OPENAI_BASE_URL"),
    temperature=1,
    timeout=60,
    )


# ==============================
# å·¥å…·ï¼šå†å² <-> messages è½¬æ¢
# ==============================
History = List[Tuple[str, str]]

def history_to_messages(hist: History) -> list:
    """å°† [(user, assistant), ...] è½¬ä¸º [{'role','content'}, ...]"""
    msgs = []
    for u, a in hist or []:
        if u is not None and u != "":
            msgs.append({"role": "user", "content": u})
        if a is not None and a != "":
            msgs.append({"role": "assistant", "content": a})
    return msgs


from math import isfinite

def _nearest_key(d: dict[float, str], v: float) -> float:
    """ä»å­—å…¸ d çš„é”®ä¸­é€‰è·ç¦» v æœ€è¿‘çš„ä¸€ä¸ªï¼ˆé”®ä¸º 0.0~1.0 çš„ç¦»æ•£ç‚¹ï¼‰"""
    keys = list(d.keys())
    # å…œåº•ï¼šå¦‚æœå­—å…¸ä¸ºç©ºï¼ˆä¸å¤ªå¯èƒ½ï¼‰ï¼Œç›´æ¥è¿”å› v å››èˆäº”å…¥åˆ° 1 ä½å°æ•°
    if not keys:
        return round(v, 1)
    # æ­£å¸¸ä»å·²æœ‰é”®é‡ŒæŒ‘æœ€è¿‘
    return min(keys, key=lambda k: abs(k - v))

def generate_dynamic_system_prompt(
    base_text: str,
    enable_base: bool,
    vals: dict[str, float],
    table: dict[str, dict[float, str]],
) -> str:
    """ç»„åˆåŸºç¡€æç¤ºè¯ + äº”ç»´äººæ ¼åˆ†æ¡£æç¤ºè¯"""
    parts = []
    if enable_base and base_text.strip():
        parts.append(base_text.strip())

    for trait in ["O", "C", "E", "A", "N"]:
        v = vals.get(trait, None)
        if v is None or not isfinite(v):
            continue
        if v < 0.0 or v > 1.0:
            raise ValueError(f"{trait} must be in [0.0, 1.0], got {v}")

        bucket = round(v, 1)
        # è‹¥è¯¥æ¡£ä¸å­˜åœ¨ï¼Œç”¨æœ€è¿‘çš„é”®å…œåº•
        if bucket not in table[trait]:
            bucket = _nearest_key(table[trait], bucket)
        parts.append(table[trait][bucket])

    return " ".join(parts).strip()

# ==============================
# æ ¸å¿ƒå¯¹è¯ï¼ˆæµå¼ï¼‰+ æ—¥å¿—
# ==============================
def stream_chat(
    message: str,
    history: History,
    system_prompt: str,
    model: str,
    temperature: float,
    max_tokens: Optional[int],
    session_id: Optional[str],
) -> Generator[str, None, None]:
    if not session_id:
        session_id = str(uuid.uuid4())

    # æ‹¼ messages
    messages = []
    if system_prompt.strip():
        messages.append({"role": "system", "content": system_prompt.strip()})
    for user_msg, assistant_msg in history:
        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
            messages.append({"role": "assistant", "content": assistant_msg})
    messages.append({"role": "user", "content": message})

    log_json(
        "round_start",
        session_id=session_id,
        model=model,
        temperature=temperature,
        max_tokens=max_tokens if max_tokens and max_tokens > 0 else None,
        system_prompt=system_prompt,
        history_turns=len(history),
    )
    log_json("user_message", session_id=session_id, text=message)

    partial = ""
    try:
        # åŒæ­¥ UI é€‰æ‹©åˆ° llmClient
        client.change_model(model)
        client.change_temperature(temperature)

        t0 = time.time()
        for i, inc in enumerate(client.chat_stream(
            messages=messages,
            temperature=temperature,
            max_tokens=max_tokens if (max_tokens and max_tokens > 0) else None,
        ), start=1):
            partial += inc
            # è¿™é‡Œæ‰“å°è¿›åº¦æ—¥å¿—ï¼šç¬¬å‡ å—ï¼Œå¢é‡é•¿åº¦ï¼Œæ€»é•¿åº¦ï¼Œè€—æ—¶
            logger.info(f"[stream_chat] chunk={i}, inc_len={len(inc)}, total_len={len(partial)}, elapsed={time.time()-t0:.2f}s")
            yield partial

        log_json("assistant_message", session_id=session_id, text=partial)
        log_json("round_end", session_id=session_id, tokens=len(partial))
        logger.success(f"[stream_chat] finished, total_len={len(partial)}, elapsed={time.time()-t0:.2f}s")
    except Exception as e:
        err = f"{type(e).__name__}: {e}"
        log_json("error", session_id=session_id, where="stream_chat", detail=err)
        logger.error(f"[stream_chat] error: {err}")
        yield partial + f"\n\n[Error] {err}"


# ==============================
# é…ç½® & ç¤ºä¾‹
# ==============================
DESCRIPTION = """
# Big5Tragectory èŠå¤©åŠ©æ‰‹ï¼ˆGradio + OpenAIï¼‰
"""

EXAMPLE_SYSTEM = SYSTEM_PROMPT

DEFAULT_MODELS = ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1", "gpt-3.5-turbo","gpt-5-nano-2025-08-07","gpt-5-chat-latest","o3", "o3-mini", "o1", "o1-mini"]

EXAMPLES = [
    [
        "ç”¨ 8 è¡Œä»¥å†…ä»£ç å†™ä¸€ä¸ª Python å†’æ³¡æ’åºç¤ºä¾‹ã€‚",  # message
        EXAMPLE_SYSTEM,                                  # system_prompt
        DEFAULT_MODELS[0],                               # model
        0.7,                                             # temperature
        None,                                            # max_tokens
        ""                                               # session_id å ä½ï¼ˆinit æ—¶å¡«å……ï¼‰
    ],
    [
        "æŠŠè¿™æ®µè¯ç¿»è¯‘æˆè‹±æ–‡ï¼š'å¹¶è¡Œè®¡ç®—çš„å…³é”®åœ¨äºä»»åŠ¡åˆ’åˆ†ä¸æ•°æ®å±€éƒ¨æ€§ã€‚'",
        EXAMPLE_SYSTEM,
        DEFAULT_MODELS[0],
        0.7,
        None,
        ""
    ],
]


# ==============================
# è‡ªå®šä¹‰ Blocks UIï¼ˆGradio 5 å…¼å®¹ï¼Œå¸¦å‘é€æŒ‰é’®ï¼‰
# ==============================
with gr.Blocks(css="footer {visibility: hidden}") as demo:
    gr.Markdown(DESCRIPTION)

    # Stateï¼šå†å² & ä¼šè¯ ID
    history_state: gr.State = gr.State([])       # List[Tuple[str, str]]
    session_state: gr.State = gr.State("")

    with gr.Row():
        with gr.Column(scale=3):
            system_box = gr.Textbox(
                label="System Promptï¼ˆåŸºç¡€æç¤ºè¯ï¼‰",
                value=EXAMPLE_SYSTEM,
                placeholder="å¯ä¸ºç©ºï¼›ç”¨äºé™å®šåŠ©æ‰‹è§’è‰²ä¸è¾¹ç•Œ",
                lines=6,
            )
        with gr.Column(scale=2):
            model_drop = gr.Dropdown(
                label="OpenAI æ¨¡å‹",
                choices=DEFAULT_MODELS,
                value=DEFAULT_MODELS[0],
                allow_custom_value=True,
                interactive=True,
            )
            temperature_slider = gr.Slider(
                label="temperatureï¼ˆå¤šæ ·æ€§ï¼‰",
                minimum=0.0,
                maximum=2.0,
                value=0.7,
                step=0.1,
            )
            max_tokens_box = gr.Number(
                label="max_tokensï¼ˆå›å¤ä¸Šé™ï¼Œç•™ç©º/â‰¤0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰",
                value=None,
                precision=0,
            )

    # â† åœ¨è¿™ä¸‹é¢åŠ â€œäººæ ¼é¢æ¿â€
    with gr.Accordion("ğŸ§  Personality (OCEAN)", open=True):
        with gr.Row():
            enable_base_ck = gr.Checkbox(value=True, label="å¯ç”¨åŸºç¡€æç¤ºè¯ï¼ˆä¸Šé¢çš„ System Promptï¼‰")
        with gr.Row():
            O_slider = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label="O - Openness")
            C_slider = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label="C - Conscientiousness")
            E_slider = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label="E - Extraversion")
            A_slider = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label="A - Agreeableness")
            N_slider = gr.Slider(0.0, 1.0, value=0.5, step=0.1, label="N - Neuroticism")
        dyn_prompt_preview = gr.Textbox(
            label="ğŸ§© åŠ¨æ€ç³»ç»Ÿæç¤ºè¯ï¼ˆåªè¯»é¢„è§ˆï¼‰",
            value="",
            lines=6,
            interactive=False,
        )


    session_md = gr.Markdown("")

    # Chat æ˜¾ç¤º + è¾“å…¥æ¡† + æŒ‰é’®
    chatbot = gr.Chatbot(height=520, type="messages", show_copy_button=True)
    with gr.Row():
        msg_box = gr.Textbox(placeholder="è¾“å…¥ä½ çš„é—®é¢˜ï¼ŒæŒ‰ Enter æˆ–ç‚¹ å‘é€...", lines=2, scale=8)
        send_btn = gr.Button("å‘é€", variant="primary", scale=1)
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", scale=1)

    # åˆå§‹åŒ– session_id
    def _init_session():
        sid = str(uuid.uuid4())
        md = f"**Session ID:** `{sid}`ï¼ˆæ­¤ä¼šè¯çš„æ‰€æœ‰æ¶ˆæ¯ä¼šå†™å…¥ `chat_history.jsonl`ï¼Œä¾¿äºæ£€ç´¢ï¼‰"
        log_json("session_init", session_id=sid)
        return sid, md

    def _update_dyn_prompt(base_text, enable_base, O, C, E, A, N):
        vals = {"O": O, "C": C, "E": E, "A": A, "N": N}
        try:
            return generate_dynamic_system_prompt(
                base_text=base_text,
                enable_base=bool(enable_base),
                vals=vals,
                table=big5_system_prompts_en,
            )
        except Exception as e:
            return f"[åŠ¨æ€æç¤ºè¯ç”Ÿæˆé”™è¯¯] {type(e).__name__}: {e}"

    # ç»‘å®šå˜åŒ–ï¼šä»»ä¸€æ§ä»¶å˜åŒ–å°±åˆ·æ–°é¢„è§ˆ
    for comp in [system_box, enable_base_ck, O_slider, C_slider, E_slider, A_slider, N_slider]:
        comp.change(
            _update_dyn_prompt,
            inputs=[system_box, enable_base_ck, O_slider, C_slider, E_slider, A_slider, N_slider],
            outputs=[dyn_prompt_preview],
        )

    # åˆæ¬¡åŠ è½½æ—¶ä¹Ÿè®¡ç®—ä¸€æ¬¡
    demo.load(
        _update_dyn_prompt,
        inputs=[system_box, enable_base_ck, O_slider, C_slider, E_slider, A_slider, N_slider],
        outputs=[dyn_prompt_preview],
    )
    demo.load(_init_session, inputs=None, outputs=[session_state, session_md])
    # --- æäº¤æµç¨‹ï¼šåˆ†ä¸¤æ­¥ ---

    def user_submit(user_msg: str, history: History):
        if user_msg is None:
            user_msg = ""
        # å…ˆå ä½ assistant
        history = (history or []) + [(user_msg, "")]
        messages = history_to_messages(history)
        # è¿”å›ï¼šæ¸…ç©ºè¾“å…¥æ¡†ã€æ›´æ–° history_stateï¼ˆå ä½ç‰ˆï¼‰ã€è®© Chatbot å…ˆæ˜¾ç¤ºåˆ°â€œæˆ‘è¯´å®Œäº†â€
        return gr.update(value=""), history, messages

    def bot_respond(
        history: History,
        system_prompt: str,
        model: str,
        temperature: float,
        max_tokens: Optional[int],
        session_id: str,
    ):
        if not history:
            # åŒè¾“å‡ºï¼šchatbot, history_state
            yield [], history
            return

        user_msg, _ = history[-1]
        prior = history[:-1]

        partial = ""
        for chunk in stream_chat(
            message=user_msg,
            history=prior,
            system_prompt=system_prompt,
            model=model,
            temperature=float(temperature),
            max_tokens=int(max_tokens) if max_tokens else None,
            session_id=session_id,
        ):
            partial = chunk
            cur = prior + [(user_msg, partial)]
            # ä¸€è¾¹æµå¼æ¸²æŸ“ Chatbotï¼Œä¸€è¾¹æŠŠâ€œå½“å‰ partialâ€å†™å› history_state
            yield history_to_messages(cur), cur

        # æœ€ç»ˆä¸€æ¬¡ï¼ˆç¡®ä¿å®Œæˆæ€ï¼‰ï¼ŒæŠŠæœ€ç»ˆå›å¤æŒä¹…å†™å› history_state
        final_hist = prior + [(user_msg, partial)]
        yield history_to_messages(final_hist), final_hist

    # äº‹ä»¶ç»‘å®šï¼šè¾“å…¥æ¡†å›è½¦ / å‘é€æŒ‰é’®
    msg_box.submit(
        user_submit,
        inputs=[msg_box, history_state],
        outputs=[msg_box, history_state, chatbot],
    ).then(
        bot_respond,
        inputs=[history_state, system_box, model_drop, temperature_slider, max_tokens_box, session_state],
        outputs=[chatbot, history_state],   # â† è¿™é‡Œæ”¹æˆä¸¤ä¸ªè¾“å‡º
    )

    send_btn.click(
        user_submit,
        inputs=[msg_box, history_state],
        outputs=[msg_box, history_state, chatbot],
    ).then(
        bot_respond,
        inputs=[history_state, system_box, model_drop, temperature_slider, max_tokens_box, session_state],
        outputs=[chatbot, history_state],   # â† åŒä¸Š
    )

    # æ¸…ç©ºæŒ‰é’®
    def clear_chat():
        return [], [], []  # history_state, chatbot(messages), msg_box

    clear_btn.click(
        clear_chat,
        inputs=None,
        outputs=[history_state, chatbot, msg_box],
    )

    # ç¤ºä¾‹ï¼šæŠŠ examples æ­£ç¡®æ³¨å…¥åˆ° inputs
    gr.Examples(
        examples=EXAMPLES,
        inputs=[msg_box, system_box, model_drop, temperature_slider, max_tokens_box],
        examples_per_page=8,
        label="ç¤ºä¾‹",
        cache_examples=False,
    )

    with gr.Accordion("âš™ï¸ ç¯å¢ƒä¿¡æ¯", open=False):
        gr.Markdown(
            f"""
- `OPENAI_BASE_URL`: `{os.getenv("OPENAI_BASE_URL", "") or "(æœªè®¾ç½®)"}`
- `OPENAI_API_KEY`: `{"å·²è®¾ç½®" if os.getenv("OPENAI_API_KEY") else "æœªè®¾ç½®"}`  
- æ—¥å¿—æ–‡ä»¶ï¼š`chat_history.jsonl`ï¼ˆJSON Linesï¼›è‡ªåŠ¨è½®è½¬ã€ä¿ç•™ 7 å¤©ã€å‹ç¼©ï¼‰
"""
        )

if __name__ == "__main__":
    demo.queue(max_size=64).launch(server_name="0.0.0.0", server_port=27861, share=False)
