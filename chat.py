# app.py
import os
import json
import uuid
from typing import List, Tuple, Generator, Optional

import gradio as gr
from loguru import logger
from openai import OpenAI

# ==============================
# Loguruï¼šJSON è¡Œæ—¥å¿— + è½®è½¬
# ==============================
logger.remove()
logger.add(
    "chat_history.jsonl",
    rotation="10 MB",
    retention="7 days",
    compression="zip",
    encoding="utf-8",
    enqueue=True,
    backtrace=False,
    diagnose=False,
    level="INFO",
    format="{message}",  # ç›´æ¥å†™ JSON è¡Œ
)

def log_json(event: str, **kwargs):
    logger.info(json.dumps({"event": event, **kwargs}, ensure_ascii=False))


# ==============================
# OpenAI å®¢æˆ·ç«¯
# ==============================
def build_client() -> OpenAI:
    base_url = os.getenv("OPENAI_BASE_URL")
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("ç¼ºå°‘ç¯å¢ƒå˜é‡ OPENAI_API_KEY")
    if base_url:
        return OpenAI(api_key=api_key, base_url=base_url)
    return OpenAI(api_key=api_key)

client = build_client()


# ==============================
# å·¥å…·ï¼šå†å² <-> messages è½¬æ¢
# ==============================
History = List[Tuple[str, str]]

def history_to_messages(hist: History) -> list:
    """å°† [(user, assistant), ...] è½¬ä¸º [{'role','content'}, ...]"""
    msgs = []
    for u, a in hist or []:
        if u is not None and u != "":
            msgs.append({"role": "user", "content": u})
        if a is not None and a != "":
            msgs.append({"role": "assistant", "content": a})
    return msgs


# ==============================
# æ ¸å¿ƒå¯¹è¯ï¼ˆæµå¼ï¼‰+ æ—¥å¿—
# ==============================
def stream_chat(
    message: str,
    history: History,
    system_prompt: str,
    model: str,
    temperature: float,
    max_tokens: Optional[int],
    session_id: Optional[str],
) -> Generator[str, None, None]:
    if not session_id:
        session_id = str(uuid.uuid4())

    # ç»„è£… messages
    messages = []
    if system_prompt.strip():
        messages.append({"role": "system", "content": system_prompt.strip()})
    for user_msg, assistant_msg in history:
        if user_msg:
            messages.append({"role": "user", "content": user_msg})
        if assistant_msg:
            messages.append({"role": "assistant", "content": assistant_msg})
    messages.append({"role": "user", "content": message})

    # æ—¥å¿—ï¼šæœ¬è½®å¼€å§‹ & ç”¨æˆ·æ¶ˆæ¯
    log_json(
        "round_start",
        session_id=session_id,
        model=model,
        temperature=temperature,
        max_tokens=max_tokens if max_tokens and max_tokens > 0 else None,
        system_prompt=system_prompt,
        history_turns=len(history),
    )
    log_json("user_message", session_id=session_id, text=message)

    # OpenAI Chat Completionsï¼ˆæµå¼ï¼‰
    stream = client.chat.completions.create(
        model=model,
        messages=messages,
        temperature=temperature,
        max_tokens=max_tokens if max_tokens and max_tokens > 0 else None,
        stream=True,
    )

    partial = ""
    try:
        for chunk in stream:
            choice = chunk.choices[0]
            delta = getattr(choice, "delta", None)
            if delta and delta.content:
                partial += delta.content
                yield partial
        log_json("assistant_message", session_id=session_id, text=partial)
        log_json("round_end", session_id=session_id, tokens=len(partial))
    except Exception as e:
        err = f"{type(e).__name__}: {e}"
        log_json("error", session_id=session_id, where="stream_chat", detail=err)
        yield partial + f"\n\n[Error] {err}"


# ==============================
# é…ç½® & ç¤ºä¾‹
# ==============================
DESCRIPTION = """
# ğŸ”§ Gradio Ã— OpenAI Chat Completions Ã— Loguru
- æµå¼è¾“å‡ºã€ä¸Šä¸‹æ–‡è®°å¿†ã€å¯è‡ªå®šä¹‰ç³»ç»Ÿæç¤ºè¯ä¸æ¨¡å‹  
- `chat_history.jsonl` æŒä¹…åŒ–ä¿å­˜å¯¹è¯ï¼ˆJSON Linesï¼‰ï¼Œè‡ªåŠ¨è½®è½¬/ä¿ç•™/å‹ç¼©  
- æ¯ä¸ªæµè§ˆå™¨ä¼šè¯æ‹¥æœ‰ç‹¬ç«‹ `session_id`
"""

EXAMPLE_SYSTEM = "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©ã€ä¸¥è°¨ä¸”ç®€æ´çš„ä¸­æ–‡åŠ©ç†ã€‚"
DEFAULT_MODELS = ["gpt-4o-mini", "gpt-4o", "gpt-4.1-mini", "gpt-4.1", "gpt-3.5-turbo"]

EXAMPLES = [
    [
        "ç”¨ 8 è¡Œä»¥å†…ä»£ç å†™ä¸€ä¸ª Python å†’æ³¡æ’åºç¤ºä¾‹ã€‚",  # message
        EXAMPLE_SYSTEM,                                  # system_prompt
        DEFAULT_MODELS[0],                               # model
        0.7,                                             # temperature
        None,                                            # max_tokens
        ""                                               # session_id å ä½ï¼ˆinit æ—¶å¡«å……ï¼‰
    ],
    [
        "æŠŠè¿™æ®µè¯ç¿»è¯‘æˆè‹±æ–‡ï¼š'å¹¶è¡Œè®¡ç®—çš„å…³é”®åœ¨äºä»»åŠ¡åˆ’åˆ†ä¸æ•°æ®å±€éƒ¨æ€§ã€‚'",
        EXAMPLE_SYSTEM,
        DEFAULT_MODELS[0],
        0.7,
        None,
        ""
    ],
]


# ==============================
# è‡ªå®šä¹‰ Blocks UIï¼ˆGradio 5 å…¼å®¹ï¼Œå¸¦å‘é€æŒ‰é’®ï¼‰
# ==============================
with gr.Blocks(css="footer {visibility: hidden}") as demo:
    gr.Markdown(DESCRIPTION)

    # Stateï¼šå†å² & ä¼šè¯ ID
    history_state: gr.State = gr.State([])       # List[Tuple[str, str]]
    session_state: gr.State = gr.State("")

    with gr.Row():
        with gr.Column(scale=3):
            system_box = gr.Textbox(
                label="System Promptï¼ˆç³»ç»Ÿæç¤ºè¯ï¼‰",
                value=EXAMPLE_SYSTEM,
                placeholder="å¯ä¸ºç©ºï¼›ç”¨äºé™å®šåŠ©æ‰‹è§’è‰²ä¸è¾¹ç•Œ",
                lines=6,
            )
        with gr.Column(scale=2):
            model_drop = gr.Dropdown(
                label="OpenAI æ¨¡å‹",
                choices=DEFAULT_MODELS,
                value=DEFAULT_MODELS[0],
                allow_custom_value=True,
                interactive=True,
            )
            temperature_slider = gr.Slider(
                label="temperatureï¼ˆå¤šæ ·æ€§ï¼‰",
                minimum=0.0,
                maximum=2.0,
                value=0.7,
                step=0.1,
            )
            max_tokens_box = gr.Number(
                label="max_tokensï¼ˆå›å¤ä¸Šé™ï¼Œç•™ç©º/â‰¤0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰",
                value=None,
                precision=0,
            )

    session_md = gr.Markdown("")

    # Chat æ˜¾ç¤º + è¾“å…¥æ¡† + æŒ‰é’®
    chatbot = gr.Chatbot(height=520, type="messages", show_copy_button=True)
    with gr.Row():
        msg_box = gr.Textbox(placeholder="è¾“å…¥ä½ çš„é—®é¢˜ï¼ŒæŒ‰ Enter æˆ–ç‚¹ å‘é€...", lines=2, scale=8)
        send_btn = gr.Button("å‘é€", variant="primary", scale=1)
        clear_btn = gr.Button("æ¸…ç©ºå¯¹è¯", scale=1)

    # åˆå§‹åŒ– session_id
    def _init_session():
        sid = str(uuid.uuid4())
        md = f"**Session ID:** `{sid}`ï¼ˆæ­¤ä¼šè¯çš„æ‰€æœ‰æ¶ˆæ¯ä¼šå†™å…¥ `chat_history.jsonl`ï¼Œä¾¿äºæ£€ç´¢ï¼‰"
        log_json("session_init", session_id=sid)
        return sid, md

    demo.load(_init_session, inputs=None, outputs=[session_state, session_md])

    # --- æäº¤æµç¨‹ï¼šåˆ†ä¸¤æ­¥ ---
    # 1) ç”¨æˆ·æäº¤ï¼šæ›´æ–°å†å²ï¼Œå¹¶æŠŠ messages æ˜ å°„ç»™ chatbot
    def user_submit(user_msg: str, history: History):
        if user_msg is None:
            user_msg = ""
        history = (history or []) + [(user_msg, "")]  # å…ˆå ä½ assistant
        messages = history_to_messages(history)
        return gr.update(value=""), history, messages  # æ¸…ç©ºè¾“å…¥æ¡† & åˆ·æ–° Chatbot

    # 2) æ¨¡å‹å“åº”ï¼ˆæµå¼ï¼‰ï¼šä¸æ–­äº§å‡ºæ–°çš„ messages
    def bot_respond(
        history: History,
        system_prompt: str,
        model: str,
        temperature: float,
        max_tokens: Optional[int],
        session_id: str,
    ):
        if not history:
            yield []  # nothing to show
            return

        user_msg, _ = history[-1]
        prior = history[:-1]

        partial = ""
        for chunk in stream_chat(
            message=user_msg,
            history=prior,
            system_prompt=system_prompt,
            model=model,
            temperature=float(temperature),
            max_tokens=int(max_tokens) if max_tokens else None,
            session_id=session_id,
        ):
            partial = chunk
            cur = prior + [(user_msg, partial)]
            yield history_to_messages(cur)

        # æœ€ç»ˆä¸€æ¬¡ï¼ˆç¡®ä¿å®Œæˆæ€ï¼‰
        cur = prior + [(user_msg, partial)]
        yield history_to_messages(cur)

    # äº‹ä»¶ç»‘å®šï¼šè¾“å…¥æ¡†å›è½¦ / å‘é€æŒ‰é’®
    msg_box.submit(
        user_submit,
        inputs=[msg_box, history_state],
        outputs=[msg_box, history_state, chatbot],
    ).then(
        bot_respond,
        inputs=[history_state, system_box, model_drop, temperature_slider, max_tokens_box, session_state],
        outputs=[chatbot],
    )

    send_btn.click(
        user_submit,
        inputs=[msg_box, history_state],
        outputs=[msg_box, history_state, chatbot],
    ).then(
        bot_respond,
        inputs=[history_state, system_box, model_drop, temperature_slider, max_tokens_box, session_state],
        outputs=[chatbot],
    )

    # æ¸…ç©ºæŒ‰é’®
    def clear_chat():
        return [], [], []  # history_state, chatbot(messages), msg_box

    clear_btn.click(
        clear_chat,
        inputs=None,
        outputs=[history_state, chatbot, msg_box],
    )

    # ç¤ºä¾‹ï¼šæŠŠ examples æ­£ç¡®æ³¨å…¥åˆ° inputs
    gr.Examples(
        examples=EXAMPLES,
        inputs=[msg_box, system_box, model_drop, temperature_slider, max_tokens_box, session_state],
        examples_per_page=8,
        label="ç¤ºä¾‹",
    )

    with gr.Accordion("âš™ï¸ ç¯å¢ƒä¿¡æ¯", open=False):
        gr.Markdown(
            f"""
- `OPENAI_BASE_URL`: `{os.getenv("OPENAI_BASE_URL", "") or "(æœªè®¾ç½®)"}`
- `OPENAI_API_KEY`: `{"å·²è®¾ç½®" if os.getenv("OPENAI_API_KEY") else "æœªè®¾ç½®"}`  
- æ—¥å¿—æ–‡ä»¶ï¼š`chat_history.jsonl`ï¼ˆJSON Linesï¼›è‡ªåŠ¨è½®è½¬ã€ä¿ç•™ 7 å¤©ã€å‹ç¼©ï¼‰
"""
        )

if __name__ == "__main__":
    demo.queue(max_size=64).launch(server_name="0.0.0.0", server_port=27861, share=False)
